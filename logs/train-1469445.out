Starting training job: train-internvl3_8b_OVIS_single_image_multi_image_single_
Job ID: 1469445
Allocated GPUs: 
Host: node17
================================
Multi-GPU Training Configuration:
  GPUs per node: 4
  Total nodes: 1
  Master address: node17
  Master port: 29500
Running training with config: training_configs/internvl3_8b_OVIS_single_image_multi_image_single_turn_multi_image_multi_turn_lora.yaml
================================
W1208 13:44:14.604000 3929279 torch/distributed/run.py:766] 
W1208 13:44:14.604000 3929279 torch/distributed/run.py:766] *****************************************
W1208 13:44:14.604000 3929279 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1208 13:44:14.604000 3929279 torch/distributed/run.py:766] *****************************************
[INFO|2025-12-08 13:44:29] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-08 13:44:29] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 4, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-08 13:44:29] llamafactory.hparams.parser:468 >> Process rank: 3, world size: 4, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-08 13:44:29] llamafactory.hparams.parser:468 >> Process rank: 1, world size: 4, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-08 13:44:29] llamafactory.hparams.parser:468 >> Process rank: 2, world size: 4, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:31,503 >> loading file vocab.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:31,503 >> loading file merges.txt from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:31,503 >> loading file tokenizer.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:31,503 >> loading file added_tokens.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/added_tokens.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:31,503 >> loading file special_tokens_map.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/special_tokens_map.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:31,503 >> loading file tokenizer_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:31,503 >> loading file chat_template.jinja from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-12-08 13:44:31,927 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|processing_utils.py:1116] 2025-12-08 13:44:32,712 >> loading configuration file processor_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/processor_config.json
[INFO|image_processing_base.py:383] 2025-12-08 13:44:33,040 >> loading configuration file preprocessor_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-08 13:44:33,046 >> Image processor GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "pad_size": null,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:33,292 >> loading file vocab.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:33,292 >> loading file merges.txt from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:33,292 >> loading file tokenizer.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:33,292 >> loading file added_tokens.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/added_tokens.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:33,292 >> loading file special_tokens_map.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/special_tokens_map.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:33,292 >> loading file tokenizer_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 13:44:33,292 >> loading file chat_template.jinja from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/chat_template.jinja
[INFO|tokenization_utils_base.py:2364] 2025-12-08 13:44:33,707 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:726] 2025-12-08 13:44:34,167 >> loading configuration file video_preprocessor_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-08 13:44:34,168 >> Video processor InternVLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "initial_shift": true,
  "input_data_format": null,
  "num_frames": null,
  "pad_size": null,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "height": 384,
    "width": 384
  },
  "size_divisor": null,
  "video_metadata": null,
  "video_processor_type": "InternVLVideoProcessor"
}

[INFO|processing_utils.py:1116] 2025-12-08 13:44:34,804 >> loading configuration file processor_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/processor_config.json
/home/stud/falu/code/LLaMA-Factory/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1208 13:44:35.339498962 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[INFO|processing_utils.py:1199] 2025-12-08 13:44:35,349 >> Processor InternVLProcessor:
- image_processor: GotOcr2ImageProcessorFast {
  "crop_size": null,
  "crop_to_patches": false,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "GotOcr2ImageProcessorFast",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "input_data_format": null,
  "max_patches": 12,
  "min_patches": 1,
  "pad_size": null,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "height": 448,
    "width": 448
  }
}

- tokenizer: Qwen2TokenizerFast(name_or_path='OpenGVLab/InternVL3_5-8B-HF', vocab_size=151643, model_max_length=40960, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>', '<img>', '</img>', '<IMG_CONTEXT>', '<quad>', '</quad>', '<ref>', '</ref>', '<box>', '</box>'], 'context_image_token': '<IMG_CONTEXT>', 'end_image_token': '</img>', 'start_image_token': '<img>', 'video_token': '<video>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151669: AddedToken("<img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151670: AddedToken("</img>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151671: AddedToken("<IMG_CONTEXT>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151672: AddedToken("<quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151673: AddedToken("</quad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151674: AddedToken("<ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151675: AddedToken("</ref>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151676: AddedToken("<box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151677: AddedToken("</box>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151678: AddedToken("<video>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
)
- video_processor: InternVLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "initial_shift": true,
  "input_data_format": null,
  "num_frames": null,
  "pad_size": null,
  "processor_class": "InternVLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "height": 384,
    "width": 384
  },
  "size_divisor": null,
  "video_metadata": null,
  "video_processor_type": "InternVLVideoProcessor"
}


{
  "image_seq_length": 256,
  "processor_class": "InternVLProcessor"
}

/home/stud/falu/code/LLaMA-Factory/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank3]:[W1208 13:44:35.388382755 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
/home/stud/falu/code/LLaMA-Factory/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank2]:[W1208 13:44:35.396460265 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
[INFO|2025-12-08 13:44:35] llamafactory.data.template:143 >> Add <|im_end|> to stop words.
[INFO|2025-12-08 13:44:35] llamafactory.data.loader:143 >> Loading dataset ./OVIS/train/internvl3/single_image/samples.jsonl...
Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 408 examples [00:00, 25688.66 examples/s]
Converting format of dataset (num_proc=8):   0%|          | 0/408 [00:00<?, ? examples/s]Converting format of dataset (num_proc=8):  12%|█▎        | 51/408 [00:00<00:01, 278.19 examples/s]Converting format of dataset (num_proc=8): 100%|██████████| 408/408 [00:00<00:00, 1142.65 examples/s]
[INFO|2025-12-08 13:44:36] llamafactory.data.loader:143 >> Loading dataset ./OVIS/train/internvl3/multi_image_single_turn/samples.jsonl...
Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 102 examples [00:00, 39245.85 examples/s]
Converting format of dataset (num_proc=8):   0%|          | 0/102 [00:00<?, ? examples/s]Converting format of dataset (num_proc=8):  13%|█▎        | 13/102 [00:00<00:01, 87.62 examples/s]Converting format of dataset (num_proc=8): 100%|██████████| 102/102 [00:00<00:00, 365.13 examples/s]
[INFO|2025-12-08 13:44:36] llamafactory.data.loader:143 >> Loading dataset ./OVIS/train/internvl3/multi_image_multi_turn/samples.jsonl...
Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 2752 examples [00:00, 65886.14 examples/s]
Converting format of dataset (num_proc=8):   0%|          | 0/2752 [00:00<?, ? examples/s]Converting format of dataset (num_proc=8):   3%|▎         | 91/2752 [00:00<00:05, 471.27 examples/s]Converting format of dataset (num_proc=8):  41%|████      | 1115/2752 [00:00<00:00, 4599.16 examples/s]Converting format of dataset (num_proc=8):  85%|████████▍ | 2326/2752 [00:00<00:00, 7411.14 examples/s]Converting format of dataset (num_proc=8): 100%|██████████| 2752/2752 [00:00<00:00, 4507.85 examples/s]
[INFO|2025-12-08 13:44:37] llamafactory.data.loader:143 >> Loading dataset ./OVIS/val/internvl3/single_image/samples.jsonl...
Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 80 examples [00:00, 28811.98 examples/s]
Converting format of dataset (num_proc=8):   0%|          | 0/80 [00:00<?, ? examples/s]Converting format of dataset (num_proc=8):  12%|█▎        | 10/80 [00:00<00:01, 69.86 examples/s]Converting format of dataset (num_proc=8): 100%|██████████| 80/80 [00:00<00:00, 310.03 examples/s]
[INFO|2025-12-08 13:44:38] llamafactory.data.loader:143 >> Loading dataset ./OVIS/val/internvl3/multi_image_single_turn/samples.jsonl...
Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 20 examples [00:00, 9718.04 examples/s]
Converting format of dataset (num_proc=8):   0%|          | 0/20 [00:00<?, ? examples/s]Converting format of dataset (num_proc=8):  15%|█▌        | 3/20 [00:00<00:00, 20.21 examples/s]Converting format of dataset (num_proc=8): 100%|██████████| 20/20 [00:00<00:00, 75.53 examples/s]
[INFO|2025-12-08 13:44:38] llamafactory.data.loader:143 >> Loading dataset ./OVIS/val/internvl3/multi_image_multi_turn/samples.jsonl...
Setting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 626 examples [00:00, 62356.24 examples/s]
Converting format of dataset (num_proc=8):   0%|          | 0/626 [00:00<?, ? examples/s]Converting format of dataset (num_proc=8):  11%|█         | 66/626 [00:00<00:02, 259.33 examples/s]Converting format of dataset (num_proc=8): 100%|██████████| 626/626 [00:00<00:00, 2131.87 examples/s]Converting format of dataset (num_proc=8): 100%|██████████| 626/626 [00:00<00:00, 1340.10 examples/s]
/home/stud/falu/code/LLaMA-Factory/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1208 13:44:39.567685626 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
Running tokenizer on dataset (num_proc=8):   0%|          | 0/3262 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=8):  13%|█▎        | 408/3262 [00:17<02:03, 23.02 examples/s]Running tokenizer on dataset (num_proc=8):  13%|█▎        | 408/3262 [00:29<02:03, 23.02 examples/s]Running tokenizer on dataset (num_proc=8):  25%|██▌       | 816/3262 [01:15<04:07,  9.89 examples/s]Running tokenizer on dataset (num_proc=8):  38%|███▊      | 1224/3262 [01:15<01:52, 18.14 examples/s]Running tokenizer on dataset (num_proc=8):  50%|█████     | 1631/3262 [01:16<00:55, 29.21 examples/s]Running tokenizer on dataset (num_proc=8):  62%|██████▏   | 2038/3262 [01:23<00:34, 35.96 examples/s]Running tokenizer on dataset (num_proc=8):  75%|███████▍  | 2446/3262 [01:27<00:17, 46.25 examples/s]Running tokenizer on dataset (num_proc=8):  87%|████████▋ | 2854/3262 [01:28<00:06, 63.75 examples/s]Running tokenizer on dataset (num_proc=8): 100%|██████████| 3262/3262 [01:30<00:00, 80.48 examples/s]Running tokenizer on dataset (num_proc=8): 100%|██████████| 3262/3262 [01:30<00:00, 35.89 examples/s]
training example:
input_ids:
[151644, 8948, 271, 2610, 525, 458, 6203, 304, 9124, 6109, 8660, 624, 5501, 3410, 279, 30618, 3745, 13934, 304, 279, 3561, 25, 220, 151674, 1502, 151675, 151676, 15505, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 5053, 151677, 151645, 198, 151644, 872, 198, 151669, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151670, 7379, 279, 7357, 304, 279, 3897, 2168, 13, 151645, 198, 151644, 77091, 198, 151674, 10680, 151675, 151676, 15505, 16, 16, 23, 11, 220, 16, 20, 24, 11, 220, 17, 19, 24, 11, 220, 21, 18, 23, 5053, 151677, 151645, 198]
inputs:
<|im_start|>system

You are an expert in visual scene understanding.
Please provide the bounding box coordinates in the format: <ref>label</ref><box>[[x1, y1, x2, y2]]</box><|im_end|>
<|im_start|>user
<img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img> Find the Person in the provided image.<|im_end|>
<|im_start|>assistant
<ref>Person</ref><box>[[118, 159, 249, 638]]</box><|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151674, 10680, 151675, 151676, 15505, 16, 16, 23, 11, 220, 16, 20, 24, 11, 220, 17, 19, 24, 11, 220, 21, 18, 23, 5053, 151677, 151645, 198]
labels:
<ref>Person</ref><box>[[118, 159, 249, 638]]</box><|im_end|>

Running tokenizer on dataset (num_proc=8):   0%|          | 0/726 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=8):  13%|█▎        | 91/726 [00:05<00:34, 18.18 examples/s]Running tokenizer on dataset (num_proc=8):  25%|██▌       | 182/726 [00:10<00:32, 16.66 examples/s]Running tokenizer on dataset (num_proc=8):  38%|███▊      | 273/726 [00:13<00:21, 21.47 examples/s]Running tokenizer on dataset (num_proc=8):  50%|█████     | 364/726 [00:14<00:11, 32.90 examples/s]Running tokenizer on dataset (num_proc=8):  75%|███████▌  | 545/726 [00:14<00:02, 65.11 examples/s]Running tokenizer on dataset (num_proc=8): 100%|██████████| 726/726 [00:16<00:00, 79.01 examples/s]Running tokenizer on dataset (num_proc=8): 100%|██████████| 726/726 [00:16<00:00, 44.76 examples/s]
eval example:
input_ids:
[151644, 8948, 271, 2610, 525, 458, 6203, 304, 9124, 6109, 8660, 624, 5501, 3410, 279, 30618, 3745, 13934, 304, 279, 3561, 25, 220, 151674, 1502, 151675, 151676, 15505, 87, 16, 11, 379, 16, 11, 856, 17, 11, 379, 17, 5053, 151677, 151645, 198, 151644, 872, 198, 151669, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151671, 151670, 7379, 279, 7357, 304, 279, 3897, 2168, 13, 151645, 198, 151644, 77091, 198, 151674, 10680, 151675, 151676, 15505, 15, 11, 220, 19, 16, 15, 11, 220, 24, 21, 11, 220, 24, 18, 16, 5053, 151677, 151645, 198]
inputs:
<|im_start|>system

You are an expert in visual scene understanding.
Please provide the bounding box coordinates in the format: <ref>label</ref><box>[[x1, y1, x2, y2]]</box><|im_end|>
<|im_start|>user
<img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT></img> Find the Person in the provided image.<|im_end|>
<|im_start|>assistant
<ref>Person</ref><box>[[0, 410, 96, 931]]</box><|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151674, 10680, 151675, 151676, 15505, 15, 11, 220, 19, 16, 15, 11, 220, 24, 21, 11, 220, 24, 18, 16, 5053, 151677, 151645, 198]
labels:
<ref>Person</ref><box>[[0, 410, 96, 931]]</box><|im_end|>

[INFO|configuration_utils.py:765] 2025-12-08 13:46:27,297 >> loading configuration file config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/config.json
[INFO|configuration_utils.py:839] 2025-12-08 13:46:27,310 >> Model config InternVLConfig {
  "architectures": [
    "InternVLForConditionalGeneration"
  ],
  "downsample_ratio": 0.5,
  "dtype": "bfloat16",
  "image_seq_length": 256,
  "image_token_id": 151671,
  "model_type": "internvl",
  "projector_hidden_act": "gelu",
  "text_config": {
    "_name_or_path": "/root/codespace/checkpoints/Qwen3-8B",
    "architectures": [
      "Qwen3ForCausalLM"
    ],
    "attention_bias": false,
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "debug": false,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "ep_size": 1,
    "head_dim": 128,
    "hidden_act": "silu",
    "hidden_size": 4096,
    "initializer_range": 0.02,
    "intermediate_size": 12288,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 40960,
    "max_window_layers": 36,
    "micro_forward": false,
    "model_type": "qwen3",
    "num_attention_heads": 32,
    "num_hidden_layers": 36,
    "num_key_value_heads": 8,
    "rms_norm_eps": 1e-06,
    "rope_scaling": null,
    "rope_theta": 1000000,
    "skip_checkpoint": false,
    "sliding_window": null,
    "use_cache": true,
    "use_deepep": false,
    "use_sliding_window": false,
    "vocab_size": 151936
  },
  "transformers_version": "4.57.1",
  "vision_config": {
    "architectures": [
      "InternVisionModel"
    ],
    "attention_bias": true,
    "attention_dropout": 0.0,
    "dropout": 0.0,
    "dtype": "bfloat16",
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.0,
    "hidden_size": 1024,
    "image_size": [
      448,
      448
    ],
    "initializer_factor": 0.1,
    "initializer_range": 1e-10,
    "intermediate_size": 4096,
    "layer_norm_eps": 1e-06,
    "layer_scale_init_value": 0.1,
    "model_type": "internvl_vision",
    "norm_type": "layer_norm",
    "num_attention_heads": 16,
    "num_channels": 3,
    "num_hidden_layers": 24,
    "patch_size": [
      14,
      14
    ],
    "projection_dropout": 0.0,
    "use_absolute_position_embeddings": true,
    "use_mask_token": false,
    "use_mean_pooling": true,
    "use_qk_norm": false
  },
  "vision_feature_layer": -1,
  "vision_feature_select_strategy": "default"
}

[INFO|2025-12-08 13:46:27] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.
[WARNING|logging.py:328] 2025-12-08 13:46:27,584 >> `torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
[INFO|modeling_utils.py:1172] 2025-12-08 13:46:27,858 >> loading weights file model.safetensors from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/model.safetensors.index.json
Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Fetching 4 files:  25%|██▌       | 1/4 [00:19<00:59, 19.87s/it]Fetching 4 files: 100%|██████████| 4/4 [00:19<00:00,  4.97s/it]
Fetching 4 files:  25%|██▌       | 1/4 [00:19<00:59, 19.84s/it]Fetching 4 files: 100%|██████████| 4/4 [00:19<00:00,  4.96s/it]
[INFO|modeling_utils.py:2341] 2025-12-08 13:46:47,851 >> Instantiating InternVLForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:986] 2025-12-08 13:46:47,854 >> Generate config GenerationConfig {
  "use_cache": false
}

[INFO|modeling_utils.py:2341] 2025-12-08 13:46:47,855 >> Instantiating InternVLVisionModel model under default dtype torch.bfloat16.
Fetching 4 files:  25%|██▌       | 1/4 [00:19<00:59, 19.85s/it]Fetching 4 files: 100%|██████████| 4/4 [00:19<00:00,  4.96s/it]
Fetching 4 files:  25%|██▌       | 1/4 [00:19<00:59, 19.86s/it]Fetching 4 files: 100%|██████████| 4/4 [00:19<00:00,  4.97s/it]
[INFO|modeling_utils.py:2341] 2025-12-08 13:46:47,895 >> Instantiating Qwen3Model model under default dtype torch.bfloat16.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.07s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.06s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.08s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:09<00:27,  9.07s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.95s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.94s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:17,  8.95s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:26<00:08,  8.92s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.78s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.10s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.78s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.78s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.78s/it]
[INFO|configuration_utils.py:941] 2025-12-08 13:47:19,290 >> loading configuration file generation_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--OpenGVLab--InternVL3_5-8B-HF/snapshots/741a7d03020411e666c6109218ab71e08151ef86/generation_config.json
[INFO|configuration_utils.py:986] 2025-12-08 13:47:19,290 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[INFO|dynamic_module_utils.py:423] 2025-12-08 13:47:19,407 >> Could not locate the custom_generate/generate.py inside OpenGVLab/InternVL3_5-8B-HF.
[INFO|2025-12-08 13:47:19] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.
[INFO|2025-12-08 13:47:19] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.
[INFO|2025-12-08 13:47:19] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.
[INFO|2025-12-08 13:47:19] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA
[INFO|2025-12-08 13:47:19] llamafactory.model.model_utils.visual:143 >> Set vision model not trainable: ['vision_tower'].
[INFO|2025-12-08 13:47:19] llamafactory.model.model_utils.visual:143 >> Set multi model projector not trainable: multi_modal_projector.
The model is already on multiple devices. Skipping the move to device specified in `args`.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|2025-12-08 13:47:19] llamafactory.model.loader:143 >> trainable params: 3,833,856 || all params: 8,532,152,320 || trainable%: 0.0449
The model is already on multiple devices. Skipping the move to device specified in `args`.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[WARNING|trainer.py:906] 2025-12-08 13:47:19,561 >> The model is already on multiple devices. Skipping the move to device specified in `args`.
[INFO|trainer.py:749] 2025-12-08 13:47:19,566 >> Using auto half precision backend
[WARNING|trainer.py:982] 2025-12-08 13:47:19,567 >> The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 151645, 'bos_token_id': None, 'pad_token_id': 151643}.
[INFO|trainer.py:2519] 2025-12-08 13:47:19,996 >> ***** Running training *****
[INFO|trainer.py:2520] 2025-12-08 13:47:19,996 >>   Num examples = 3,262
[INFO|trainer.py:2521] 2025-12-08 13:47:19,996 >>   Num Epochs = 2
[INFO|trainer.py:2522] 2025-12-08 13:47:19,996 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2525] 2025-12-08 13:47:19,996 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:2526] 2025-12-08 13:47:19,996 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:2527] 2025-12-08 13:47:19,997 >>   Total optimization steps = 102
[INFO|trainer.py:2528] 2025-12-08 13:47:19,999 >>   Number of trainable parameters = 3,833,856
[INFO|integration_utils.py:867] 2025-12-08 13:47:21,796 >> Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: luca-fanselau (se3-thesis) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run tp4jxysn
wandb: Tracking run with wandb version 0.23.1
wandb: Run data is saved locally in /home/stud/falu/code/LLaMA-Factory/wandb/run-20251208_134722-tp4jxysn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run internvl3_8b_OVIS_single_image_multi_image_single_turn_multi_image_multi_turn
wandb: ⭐️ View project at https://wandb.ai/se3-thesis/llamafactory
wandb: 🚀 View run at https://wandb.ai/se3-thesis/llamafactory/runs/tp4jxysn
  0%|          | 0/102 [00:00<?, ?it/s]  1%|          | 1/102 [00:23<38:53, 23.10s/it]  2%|▏         | 2/102 [00:43<35:36, 21.37s/it]  3%|▎         | 3/102 [01:04<35:30, 21.52s/it]  4%|▍         | 4/102 [01:24<34:04, 20.86s/it]  5%|▍         | 5/102 [01:46<34:13, 21.17s/it]  6%|▌         | 6/102 [02:07<33:56, 21.22s/it]  7%|▋         | 7/102 [02:28<33:09, 20.94s/it]  8%|▊         | 8/102 [02:49<32:54, 21.01s/it]  9%|▉         | 9/102 [03:11<32:54, 21.24s/it] 10%|▉         | 10/102 [03:33<33:09, 21.62s/it]                                                {'loss': 1.8961, 'grad_norm': 0.925090491771698, 'learning_rate': 1.6363636363636366e-05, 'epoch': 0.2}
 10%|▉         | 10/102 [03:33<33:09, 21.62s/it] 11%|█         | 11/102 [03:53<31:59, 21.09s/it] 12%|█▏        | 12/102 [04:15<32:16, 21.51s/it] 13%|█▎        | 13/102 [04:37<31:52, 21.49s/it] 14%|█▎        | 14/102 [04:59<31:51, 21.72s/it] 15%|█▍        | 15/102 [05:20<31:13, 21.53s/it] 16%|█▌        | 16/102 [05:43<31:20, 21.86s/it] 17%|█▋        | 17/102 [06:06<31:32, 22.26s/it] 18%|█▊        | 18/102 [06:28<30:56, 22.11s/it] 19%|█▊        | 19/102 [06:49<30:21, 21.94s/it] 20%|█▉        | 20/102 [07:11<29:50, 21.83s/it]                                                {'loss': 1.8552, 'grad_norm': 1.2499743700027466, 'learning_rate': 1.9621030198436007e-05, 'epoch': 0.39}
 20%|█▉        | 20/102 [07:11<29:50, 21.83s/it] 21%|██        | 21/102 [07:32<29:05, 21.54s/it] 22%|██▏       | 22/102 [07:54<28:52, 21.66s/it] 23%|██▎       | 23/102 [08:16<28:37, 21.74s/it] 24%|██▎       | 24/102 [08:38<28:35, 21.99s/it] 25%|██▍       | 25/102 [09:01<28:27, 22.18s/it] 25%|██▌       | 26/102 [09:22<27:41, 21.86s/it] 26%|██▋       | 27/102 [09:43<27:06, 21.68s/it] 27%|██▋       | 28/102 [10:06<27:05, 21.96s/it] 28%|██▊       | 29/102 [10:28<26:52, 22.08s/it] 29%|██▉       | 30/102 [10:50<26:21, 21.97s/it]                                                {'loss': 1.7249, 'grad_norm': 1.53233802318573, 'learning_rate': 1.8130560994785325e-05, 'epoch': 0.59}
 29%|██▉       | 30/102 [10:50<26:21, 21.97s/it] 30%|███       | 31/102 [11:11<25:45, 21.76s/it] 31%|███▏      | 32/102 [11:33<25:29, 21.85s/it] 32%|███▏      | 33/102 [11:55<25:01, 21.75s/it] 33%|███▎      | 34/102 [12:15<24:13, 21.38s/it] 34%|███▍      | 35/102 [12:37<24:07, 21.60s/it] 35%|███▌      | 36/102 [12:59<23:38, 21.50s/it] 36%|███▋      | 37/102 [13:19<22:57, 21.18s/it] 37%|███▋      | 38/102 [13:41<22:44, 21.33s/it] 38%|███▊      | 39/102 [14:02<22:20, 21.29s/it] 39%|███▉      | 40/102 [14:24<22:09, 21.44s/it]                                                {'loss': 1.5386, 'grad_norm': 1.666410207748413, 'learning_rate': 1.568064746731156e-05, 'epoch': 0.78}
 39%|███▉      | 40/102 [14:24<22:09, 21.44s/it] 40%|████      | 41/102 [14:44<21:29, 21.15s/it] 41%|████      | 42/102 [15:05<21:01, 21.02s/it] 42%|████▏     | 43/102 [15:26<20:47, 21.15s/it] 43%|████▎     | 44/102 [15:48<20:38, 21.36s/it] 44%|████▍     | 45/102 [16:10<20:21, 21.43s/it] 45%|████▌     | 46/102 [16:32<20:15, 21.71s/it] 46%|████▌     | 47/102 [16:54<19:53, 21.71s/it] 47%|████▋     | 48/102 [17:16<19:34, 21.75s/it] 48%|████▊     | 49/102 [17:37<19:03, 21.58s/it] 49%|████▉     | 50/102 [17:59<18:52, 21.78s/it]                                                {'loss': 1.3548, 'grad_norm': 1.9050021171569824, 'learning_rate': 1.2560390900575472e-05, 'epoch': 0.98}
 49%|████▉     | 50/102 [17:59<18:52, 21.78s/it] 50%|█████     | 51/102 [18:22<18:39, 21.95s/it] 51%|█████     | 52/102 [18:43<18:15, 21.90s/it] 52%|█████▏    | 53/102 [19:06<17:59, 22.04s/it] 53%|█████▎    | 54/102 [19:27<17:33, 21.95s/it] 54%|█████▍    | 55/102 [19:49<17:05, 21.82s/it] 55%|█████▍    | 56/102 [20:11<16:52, 22.02s/it] 56%|█████▌    | 57/102 [20:32<16:12, 21.60s/it] 57%|█████▋    | 58/102 [20:53<15:45, 21.50s/it] 58%|█████▊    | 59/102 [21:15<15:31, 21.67s/it] 59%|█████▉    | 60/102 [21:37<15:10, 21.67s/it]                                                {'loss': 1.2159, 'grad_norm': 2.150097131729126, 'learning_rate': 9.137996201193807e-06, 'epoch': 1.18}
 59%|█████▉    | 60/102 [21:37<15:10, 21.67s/it] 60%|█████▉    | 61/102 [21:59<14:47, 21.66s/it] 61%|██████    | 62/102 [22:20<14:26, 21.66s/it] 62%|██████▏   | 63/102 [22:43<14:11, 21.83s/it] 63%|██████▎   | 64/102 [23:04<13:49, 21.84s/it] 64%|██████▎   | 65/102 [23:26<13:22, 21.70s/it] 65%|██████▍   | 66/102 [23:48<13:08, 21.91s/it] 66%|██████▌   | 67/102 [24:10<12:46, 21.91s/it] 67%|██████▋   | 68/102 [24:32<12:26, 21.96s/it] 68%|██████▊   | 69/102 [24:55<12:15, 22.29s/it] 69%|██████▊   | 70/102 [25:17<11:45, 22.06s/it]                                                {'loss': 1.1003, 'grad_norm': 2.314500570297241, 'learning_rate': 5.8173219922443516e-06, 'epoch': 1.37}
 69%|██████▊   | 70/102 [25:17<11:45, 22.06s/it] 70%|██████▉   | 71/102 [25:38<11:18, 21.87s/it] 71%|███████   | 72/102 [26:00<10:59, 21.97s/it] 72%|███████▏  | 73/102 [26:22<10:32, 21.82s/it] 73%|███████▎  | 74/102 [26:43<10:04, 21.59s/it] 74%|███████▎  | 75/102 [27:04<09:36, 21.35s/it] 75%|███████▍  | 76/102 [27:25<09:14, 21.31s/it] 75%|███████▌  | 77/102 [27:47<08:56, 21.45s/it] 76%|███████▋  | 78/102 [28:09<08:43, 21.83s/it] 77%|███████▋  | 79/102 [28:31<08:20, 21.76s/it] 78%|███████▊  | 80/102 [28:51<07:44, 21.13s/it]                                                {'loss': 1.0121, 'grad_norm': 2.472822427749634, 'learning_rate': 2.9902234019385056e-06, 'epoch': 1.57}
 78%|███████▊  | 80/102 [28:51<07:44, 21.13s/it] 79%|███████▉  | 81/102 [29:12<07:24, 21.18s/it] 80%|████████  | 82/102 [29:33<07:03, 21.16s/it] 81%|████████▏ | 83/102 [29:55<06:45, 21.36s/it] 82%|████████▏ | 84/102 [30:18<06:31, 21.78s/it] 83%|████████▎ | 85/102 [30:38<06:04, 21.42s/it] 84%|████████▍ | 86/102 [31:00<05:45, 21.59s/it] 85%|████████▌ | 87/102 [31:21<05:22, 21.47s/it] 86%|████████▋ | 88/102 [31:43<05:02, 21.58s/it] 87%|████████▋ | 89/102 [32:06<04:45, 21.94s/it] 88%|████████▊ | 90/102 [32:29<04:25, 22.12s/it]                                                {'loss': 0.9811, 'grad_norm': 2.462092161178589, 'learning_rate': 9.903113209758098e-07, 'epoch': 1.76}
 88%|████████▊ | 90/102 [32:29<04:25, 22.12s/it] 89%|████████▉ | 91/102 [32:51<04:02, 22.06s/it] 90%|█████████ | 92/102 [33:11<03:35, 21.50s/it] 91%|█████████ | 93/102 [33:31<03:09, 21.06s/it] 92%|█████████▏| 94/102 [33:53<02:52, 21.53s/it] 93%|█████████▎| 95/102 [34:16<02:33, 21.91s/it] 94%|█████████▍| 96/102 [34:38<02:11, 21.89s/it] 95%|█████████▌| 97/102 [34:59<01:48, 21.70s/it] 96%|█████████▌| 98/102 [35:21<01:26, 21.69s/it] 97%|█████████▋| 99/102 [35:42<01:04, 21.52s/it] 98%|█████████▊| 100/102 [36:02<00:41, 20.97s/it]                                                 {'loss': 0.9738, 'grad_norm': 2.4651968479156494, 'learning_rate': 5.3584753048073756e-08, 'epoch': 1.96}
 98%|█████████▊| 100/102 [36:02<00:41, 20.97s/it] 99%|█████████▉| 101/102 [36:23<00:21, 21.11s/it]100%|██████████| 102/102 [36:46<00:00, 21.56s/it][INFO|trainer.py:4309] 2025-12-08 14:24:09,592 >> Saving model checkpoint to /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102
[INFO|tokenization_utils_base.py:2421] 2025-12-08 14:24:09,920 >> chat template saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-08 14:24:09,921 >> tokenizer config file saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-08 14:24:09,922 >> Special tokens file saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/special_tokens_map.json
[INFO|image_processing_base.py:253] 2025-12-08 14:24:10,191 >> Image processor saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-08 14:24:10,192 >> chat template saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-08 14:24:10,193 >> tokenizer config file saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-08 14:24:10,194 >> Special tokens file saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-08 14:24:10,358 >> Video processor saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-08 14:24:10,359 >> chat template saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/chat_template.jinja
[INFO|processing_utils.py:853] 2025-12-08 14:24:11,226 >> processor saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/checkpoint-102/processor_config.json
[INFO|trainer.py:2810] 2025-12-08 14:24:11,226 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 2211.2267, 'train_samples_per_second': 2.95, 'train_steps_per_second': 0.046, 'train_loss': 1.3570434357605727, 'epoch': 2.0}
100%|██████████| 102/102 [36:47<00:00, 21.56s/it]100%|██████████| 102/102 [36:47<00:00, 21.65s/it]
[INFO|image_processing_base.py:253] 2025-12-08 14:24:11,231 >> Image processor saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/preprocessor_config.json
[INFO|tokenization_utils_base.py:2421] 2025-12-08 14:24:11,232 >> chat template saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-08 14:24:11,233 >> tokenizer config file saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-08 14:24:11,234 >> Special tokens file saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/special_tokens_map.json
[INFO|video_processing_utils.py:600] 2025-12-08 14:24:11,425 >> Video processor saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/video_preprocessor_config.json
[INFO|processing_utils.py:814] 2025-12-08 14:24:11,426 >> chat template saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/chat_template.jinja
[INFO|processing_utils.py:853] 2025-12-08 14:24:12,279 >> processor saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/processor_config.json
[INFO|trainer.py:4309] 2025-12-08 14:24:12,281 >> Saving model checkpoint to /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn
[INFO|tokenization_utils_base.py:2421] 2025-12-08 14:24:12,478 >> chat template saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/chat_template.jinja
[INFO|tokenization_utils_base.py:2590] 2025-12-08 14:24:12,480 >> tokenizer config file saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/tokenizer_config.json
[INFO|tokenization_utils_base.py:2599] 2025-12-08 14:24:12,481 >> Special tokens file saved in /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/special_tokens_map.json
***** train metrics *****
  epoch                    =         2.0
  total_flos               = 348523573GF
  train_loss               =       1.357
  train_runtime            =  0:36:51.22
  train_samples_per_second =        2.95
  train_steps_per_second   =       0.046
Figure saved at: /storage/user/falu/trained_models/internvl3_8b/lora/OVIS_single_image_multi_image_single_turn_multi_image_multi_turn/training_loss.png
[WARNING|2025-12-08 14:24:12] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.
[WARNING|2025-12-08 14:24:12] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4643] 2025-12-08 14:24:12,859 >> 
***** Running Evaluation *****
[INFO|trainer.py:4645] 2025-12-08 14:24:12,859 >>   Num examples = 726
[INFO|trainer.py:4648] 2025-12-08 14:24:12,860 >>   Batch size = 1
  0%|          | 0/182 [00:00<?, ?it/s]  1%|          | 2/182 [00:00<00:10, 17.92it/s]  2%|▏         | 4/182 [00:00<00:15, 11.59it/s]  3%|▎         | 6/182 [00:00<00:17, 10.22it/s]  4%|▍         | 8/182 [00:00<00:17,  9.81it/s]  5%|▌         | 10/182 [00:00<00:17,  9.60it/s]  6%|▌         | 11/182 [00:01<00:18,  9.47it/s]  7%|▋         | 12/182 [00:01<00:18,  9.35it/s]  7%|▋         | 13/182 [00:01<00:18,  9.25it/s]  8%|▊         | 14/182 [00:01<00:18,  9.30it/s]  8%|▊         | 15/182 [00:01<00:18,  9.20it/s]  9%|▉         | 16/182 [00:01<00:18,  9.12it/s]  9%|▉         | 17/182 [00:01<00:18,  9.08it/s] 10%|▉         | 18/182 [00:01<00:18,  9.04it/s] 10%|█         | 19/182 [00:01<00:17,  9.14it/s] 11%|█         | 20/182 [00:02<00:17,  9.10it/s] 12%|█▏        | 21/182 [00:02<00:28,  5.72it/s] 12%|█▏        | 22/182 [00:02<00:35,  4.52it/s] 13%|█▎        | 23/182 [00:03<00:40,  3.94it/s] 13%|█▎        | 24/182 [00:03<00:43,  3.61it/s] 14%|█▎        | 25/182 [00:03<00:45,  3.41it/s] 14%|█▍        | 26/182 [00:04<00:50,  3.11it/s] 15%|█▍        | 27/182 [00:04<00:52,  2.94it/s] 15%|█▌        | 28/182 [00:04<00:54,  2.82it/s] 16%|█▌        | 29/182 [00:05<00:55,  2.74it/s] 16%|█▋        | 30/182 [00:05<00:56,  2.69it/s] 17%|█▋        | 31/182 [00:06<00:56,  2.66it/s] 18%|█▊        | 32/182 [00:06<00:56,  2.63it/s] 18%|█▊        | 33/182 [00:06<00:56,  2.62it/s] 19%|█▊        | 34/182 [00:07<00:56,  2.61it/s] 19%|█▉        | 35/182 [00:07<00:56,  2.60it/s] 20%|█▉        | 36/182 [00:08<00:56,  2.59it/s] 20%|██        | 37/182 [00:08<00:56,  2.59it/s] 21%|██        | 38/182 [00:08<00:55,  2.58it/s] 21%|██▏       | 39/182 [00:09<00:55,  2.58it/s] 22%|██▏       | 40/182 [00:09<00:55,  2.58it/s] 23%|██▎       | 41/182 [00:09<00:54,  2.58it/s] 23%|██▎       | 42/182 [00:10<00:54,  2.57it/s] 24%|██▎       | 43/182 [00:10<00:53,  2.57it/s] 24%|██▍       | 44/182 [00:11<00:53,  2.58it/s] 25%|██▍       | 45/182 [00:11<00:53,  2.58it/s] 25%|██▌       | 46/182 [00:11<00:52,  2.58it/s] 26%|██▌       | 47/182 [00:12<00:52,  2.57it/s] 26%|██▋       | 48/182 [00:12<00:52,  2.57it/s] 27%|██▋       | 49/182 [00:13<00:51,  2.57it/s] 27%|██▋       | 50/182 [00:13<00:51,  2.57it/s] 28%|██▊       | 51/182 [00:13<00:50,  2.57it/s] 29%|██▊       | 52/182 [00:14<00:50,  2.58it/s] 29%|██▉       | 53/182 [00:14<00:50,  2.58it/s] 30%|██▉       | 54/182 [00:14<00:49,  2.58it/s] 30%|███       | 55/182 [00:15<00:49,  2.58it/s] 31%|███       | 56/182 [00:15<00:48,  2.58it/s] 31%|███▏      | 57/182 [00:16<00:48,  2.57it/s] 32%|███▏      | 58/182 [00:16<00:48,  2.57it/s] 32%|███▏      | 59/182 [00:16<00:47,  2.57it/s] 33%|███▎      | 60/182 [00:17<00:47,  2.57it/s] 34%|███▎      | 61/182 [00:17<00:47,  2.57it/s] 34%|███▍      | 62/182 [00:18<00:46,  2.57it/s] 35%|███▍      | 63/182 [00:18<00:46,  2.56it/s] 35%|███▌      | 64/182 [00:18<00:45,  2.57it/s] 36%|███▌      | 65/182 [00:19<00:45,  2.57it/s] 36%|███▋      | 66/182 [00:19<00:45,  2.57it/s] 37%|███▋      | 67/182 [00:20<00:44,  2.57it/s] 37%|███▋      | 68/182 [00:20<00:44,  2.57it/s] 38%|███▊      | 69/182 [00:20<00:43,  2.57it/s] 38%|███▊      | 70/182 [00:21<00:43,  2.57it/s] 39%|███▉      | 71/182 [00:21<00:43,  2.57it/s] 40%|███▉      | 72/182 [00:22<00:42,  2.56it/s] 40%|████      | 73/182 [00:22<00:42,  2.57it/s] 41%|████      | 74/182 [00:22<00:42,  2.57it/s] 41%|████      | 75/182 [00:23<00:41,  2.57it/s] 42%|████▏     | 76/182 [00:23<00:41,  2.57it/s] 42%|████▏     | 77/182 [00:23<00:40,  2.57it/s] 43%|████▎     | 78/182 [00:24<00:40,  2.57it/s] 43%|████▎     | 79/182 [00:24<00:40,  2.57it/s] 44%|████▍     | 80/182 [00:25<00:39,  2.57it/s] 45%|████▍     | 81/182 [00:25<00:39,  2.57it/s] 45%|████▌     | 82/182 [00:25<00:38,  2.57it/s] 46%|████▌     | 83/182 [00:26<00:38,  2.57it/s] 46%|████▌     | 84/182 [00:26<00:38,  2.57it/s] 47%|████▋     | 85/182 [00:27<00:37,  2.57it/s] 47%|████▋     | 86/182 [00:27<00:37,  2.57it/s] 48%|████▊     | 87/182 [00:27<00:36,  2.57it/s] 48%|████▊     | 88/182 [00:28<00:36,  2.57it/s] 49%|████▉     | 89/182 [00:28<00:36,  2.57it/s] 49%|████▉     | 90/182 [00:29<00:35,  2.57it/s] 50%|█████     | 91/182 [00:29<00:35,  2.57it/s] 51%|█████     | 92/182 [00:29<00:35,  2.57it/s] 51%|█████     | 93/182 [00:30<00:34,  2.57it/s] 52%|█████▏    | 94/182 [00:30<00:34,  2.57it/s] 52%|█████▏    | 95/182 [00:30<00:33,  2.57it/s] 53%|█████▎    | 96/182 [00:31<00:33,  2.57it/s] 53%|█████▎    | 97/182 [00:31<00:33,  2.57it/s] 54%|█████▍    | 98/182 [00:32<00:32,  2.57it/s] 54%|█████▍    | 99/182 [00:32<00:32,  2.57it/s] 55%|█████▍    | 100/182 [00:32<00:31,  2.57it/s] 55%|█████▌    | 101/182 [00:33<00:31,  2.57it/s] 56%|█████▌    | 102/182 [00:33<00:31,  2.57it/s] 57%|█████▋    | 103/182 [00:34<00:30,  2.57it/s] 57%|█████▋    | 104/182 [00:34<00:30,  2.57it/s] 58%|█████▊    | 105/182 [00:34<00:30,  2.57it/s] 58%|█████▊    | 106/182 [00:35<00:29,  2.57it/s] 59%|█████▉    | 107/182 [00:35<00:29,  2.57it/s] 59%|█████▉    | 108/182 [00:36<00:28,  2.57it/s] 60%|█████▉    | 109/182 [00:36<00:28,  2.57it/s] 60%|██████    | 110/182 [00:36<00:28,  2.57it/s] 61%|██████    | 111/182 [00:37<00:27,  2.57it/s] 62%|██████▏   | 112/182 [00:37<00:27,  2.57it/s] 62%|██████▏   | 113/182 [00:37<00:26,  2.57it/s] 63%|██████▎   | 114/182 [00:38<00:26,  2.56it/s] 63%|██████▎   | 115/182 [00:38<00:26,  2.57it/s] 64%|██████▎   | 116/182 [00:39<00:25,  2.57it/s] 64%|██████▍   | 117/182 [00:39<00:25,  2.57it/s] 65%|██████▍   | 118/182 [00:39<00:24,  2.57it/s] 65%|██████▌   | 119/182 [00:40<00:24,  2.57it/s] 66%|██████▌   | 120/182 [00:40<00:24,  2.57it/s] 66%|██████▋   | 121/182 [00:41<00:23,  2.57it/s] 67%|██████▋   | 122/182 [00:41<00:23,  2.56it/s] 68%|██████▊   | 123/182 [00:41<00:22,  2.57it/s] 68%|██████▊   | 124/182 [00:42<00:22,  2.56it/s] 69%|██████▊   | 125/182 [00:42<00:22,  2.57it/s] 69%|██████▉   | 126/182 [00:43<00:21,  2.56it/s] 70%|██████▉   | 127/182 [00:43<00:21,  2.56it/s] 70%|███████   | 128/182 [00:43<00:21,  2.56it/s] 71%|███████   | 129/182 [00:44<00:20,  2.56it/s] 71%|███████▏  | 130/182 [00:44<00:20,  2.56it/s] 72%|███████▏  | 131/182 [00:44<00:19,  2.57it/s] 73%|███████▎  | 132/182 [00:45<00:19,  2.57it/s] 73%|███████▎  | 133/182 [00:45<00:19,  2.57it/s] 74%|███████▎  | 134/182 [00:46<00:18,  2.57it/s] 74%|███████▍  | 135/182 [00:46<00:18,  2.57it/s] 75%|███████▍  | 136/182 [00:46<00:17,  2.57it/s] 75%|███████▌  | 137/182 [00:47<00:17,  2.57it/s] 76%|███████▌  | 138/182 [00:47<00:17,  2.57it/s] 76%|███████▋  | 139/182 [00:48<00:16,  2.57it/s] 77%|███████▋  | 140/182 [00:48<00:16,  2.57it/s] 77%|███████▋  | 141/182 [00:48<00:15,  2.57it/s] 78%|███████▊  | 142/182 [00:49<00:15,  2.57it/s] 79%|███████▊  | 143/182 [00:49<00:15,  2.57it/s] 79%|███████▉  | 144/182 [00:50<00:14,  2.57it/s] 80%|███████▉  | 145/182 [00:50<00:14,  2.57it/s] 80%|████████  | 146/182 [00:50<00:14,  2.57it/s] 81%|████████  | 147/182 [00:51<00:13,  2.57it/s] 81%|████████▏ | 148/182 [00:51<00:13,  2.57it/s] 82%|████████▏ | 149/182 [00:51<00:12,  2.57it/s] 82%|████████▏ | 150/182 [00:52<00:12,  2.57it/s] 83%|████████▎ | 151/182 [00:52<00:12,  2.56it/s] 84%|████████▎ | 152/182 [00:53<00:11,  2.56it/s] 84%|████████▍ | 153/182 [00:53<00:11,  2.56it/s] 85%|████████▍ | 154/182 [00:53<00:10,  2.56it/s] 85%|████████▌ | 155/182 [00:54<00:10,  2.56it/s] 86%|████████▌ | 156/182 [00:54<00:10,  2.56it/s] 86%|████████▋ | 157/182 [00:55<00:09,  2.56it/s] 87%|████████▋ | 158/182 [00:55<00:09,  2.56it/s] 87%|████████▋ | 159/182 [00:55<00:08,  2.56it/s] 88%|████████▊ | 160/182 [00:56<00:08,  2.56it/s] 88%|████████▊ | 161/182 [00:56<00:08,  2.56it/s] 89%|████████▉ | 162/182 [00:57<00:07,  2.56it/s] 90%|████████▉ | 163/182 [00:57<00:07,  2.56it/s] 90%|█████████ | 164/182 [00:57<00:07,  2.56it/s] 91%|█████████ | 165/182 [00:58<00:06,  2.56it/s] 91%|█████████ | 166/182 [00:58<00:06,  2.56it/s] 92%|█████████▏| 167/182 [00:59<00:05,  2.56it/s] 92%|█████████▏| 168/182 [00:59<00:05,  2.56it/s] 93%|█████████▎| 169/182 [00:59<00:05,  2.56it/s] 93%|█████████▎| 170/182 [01:00<00:04,  2.56it/s] 94%|█████████▍| 171/182 [01:00<00:04,  2.56it/s] 95%|█████████▍| 172/182 [01:00<00:03,  2.56it/s] 95%|█████████▌| 173/182 [01:01<00:03,  2.56it/s] 96%|█████████▌| 174/182 [01:01<00:03,  2.56it/s] 96%|█████████▌| 175/182 [01:02<00:02,  2.56it/s] 97%|█████████▋| 176/182 [01:02<00:02,  2.56it/s] 97%|█████████▋| 177/182 [01:02<00:01,  2.56it/s] 98%|█████████▊| 178/182 [01:03<00:01,  2.56it/s] 98%|█████████▊| 179/182 [01:03<00:01,  2.56it/s] 99%|█████████▉| 180/182 [01:04<00:00,  2.56it/s] 99%|█████████▉| 181/182 [01:04<00:00,  2.56it/s]100%|██████████| 182/182 [01:04<00:00,  2.37it/s]100%|██████████| 182/182 [01:04<00:00,  2.80it/s]
***** eval metrics *****
  epoch                   =        2.0
  eval_loss               =     0.9453
  eval_runtime            = 0:01:05.42
  eval_samples_per_second =     11.097
  eval_steps_per_second   =      2.782
[INFO|modelcard.py:456] 2025-12-08 14:25:18,287 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[1;34mwandb[0m: 
[1;34mwandb[0m: 🚀 View run [33minternvl3_8b_OVIS_single_image_multi_image_single_turn_multi_image_multi_turn[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20251208_134722-tp4jxysn/logs[0m
✓ Training completed successfully
Job finished at: Mon Dec  8 02:25:24 PM CET 2025
