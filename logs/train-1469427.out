Starting training job: train-qwen3_vl_2b_all_single_image_multi_image_single_tu
Job ID: 1469427
Allocated GPUs: 
Host: node17
================================
Multi-GPU Training Configuration:
  GPUs per node: 2
  Total nodes: 1
  Master address: node17
  Master port: 29500
Running training with config: training_configs/qwen3_vl_2b_all_single_image_multi_image_single_turn_multi_image_multi_turn_qlora.yaml
================================
W1208 12:40:49.182000 3199731 torch/distributed/run.py:766] 
W1208 12:40:49.182000 3199731 torch/distributed/run.py:766] *****************************************
W1208 12:40:49.182000 3199731 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1208 12:40:49.182000 3199731 torch/distributed/run.py:766] *****************************************
[WARNING|2025-12-08 12:40:58] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.
[INFO|2025-12-08 12:40:58] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.
[INFO|2025-12-08 12:40:58] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16
[INFO|2025-12-08 12:40:58] llamafactory.hparams.parser:468 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:40:58,959 >> loading file vocab.json from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:40:58,959 >> loading file merges.txt from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:40:58,959 >> loading file tokenizer.json from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:40:58,959 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:40:58,960 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:40:58,960 >> loading file tokenizer_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:40:58,960 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-08 12:40:59,296 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|image_processing_base.py:383] 2025-12-08 12:40:59,792 >> loading configuration file preprocessor_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/preprocessor_config.json
[INFO|image_processing_base.py:383] 2025-12-08 12:41:00,046 >> loading configuration file preprocessor_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/preprocessor_config.json
[INFO|image_processing_base.py:428] 2025-12-08 12:41:00,055 >> Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:41:00,301 >> loading file vocab.json from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/vocab.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:41:00,301 >> loading file merges.txt from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/merges.txt
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:41:00,301 >> loading file tokenizer.json from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/tokenizer.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:41:00,301 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:41:00,301 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:41:00,301 >> loading file tokenizer_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/tokenizer_config.json
[INFO|tokenization_utils_base.py:2095] 2025-12-08 12:41:00,301 >> loading file chat_template.jinja from cache at None
[INFO|tokenization_utils_base.py:2364] 2025-12-08 12:41:00,594 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|video_processing_utils.py:726] 2025-12-08 12:41:01,015 >> loading configuration file video_preprocessor_config.json from cache at /storage/user/falu/.cache/huggingface/hub/models--Qwen--Qwen3-VL-2B-Instruct/snapshots/89644892e4d85e24eaac8bacfd4f463576704203/video_preprocessor_config.json
[INFO|video_processing_utils.py:770] 2025-12-08 12:41:01,016 >> Video processor Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}

[INFO|processing_utils.py:1116] 2025-12-08 12:41:01,621 >> loading configuration file processor_config.json from cache at None
[INFO|processing_utils.py:1199] 2025-12-08 12:41:02,234 >> Processor Qwen3VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_pixels": null,
  "merge_size": 2,
  "min_pixels": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 16777216,
    "shortest_edge": 65536
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen3-VL-2B-Instruct', vocab_size=151643, model_max_length=262144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151665: AddedToken("<tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151666: AddedToken("</tool_response>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151667: AddedToken("<think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151668: AddedToken("</think>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen3VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": true,
  "fps": 2,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "input_data_format": null,
  "max_frames": 768,
  "merge_size": 2,
  "min_frames": 4,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 16,
  "processor_class": "Qwen3VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 25165824,
    "shortest_edge": 4096
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen3VLVideoProcessor"
}


{
  "processor_class": "Qwen3VLProcessor"
}

[INFO|2025-12-08 12:41:02] llamafactory.data.loader:143 >> Loading dataset ./OVIS/train/qwen3/single_image/samples.jsonl...
/home/stud/falu/code/LLaMA-Factory/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank1]:[W1208 12:41:02.423102622 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
Converting format of dataset (num_proc=16): 100%|██████████| 408/408 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 434 examples [00:00, 64.43 examples/s]          Converting format of dataset (num_proc=16): 816 examples [00:00, 699.66 examples/s]
[INFO|2025-12-08 12:41:03] llamafactory.data.loader:143 >> Loading dataset ./OVIS/train/qwen3/multi_image_single_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 102/102 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 109 examples [00:00, 20.11 examples/s]          Converting format of dataset (num_proc=16): 204 examples [00:00, 194.81 examples/s]
[INFO|2025-12-08 12:41:04] llamafactory.data.loader:143 >> Loading dataset ./OVIS/train/qwen3/multi_image_multi_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 2752/2752 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 2794 examples [00:00, 114.23 examples/s]          Converting format of dataset (num_proc=16): 4511 examples [00:00, 4885.93 examples/s]Converting format of dataset (num_proc=16): 5504 examples [00:00, 4146.49 examples/s]
[INFO|2025-12-08 12:41:04] llamafactory.data.loader:143 >> Loading dataset ./LVVis/train/qwen3/single_image/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 34836/34836 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 35041 examples [00:00, 523.63 examples/s]           Converting format of dataset (num_proc=16): 41722 examples [00:00, 17962.22 examples/s]Converting format of dataset (num_proc=16): 48133 examples [00:00, 30375.84 examples/s]Converting format of dataset (num_proc=16): 53846 examples [00:00, 37840.65 examples/s]Converting format of dataset (num_proc=16): 59762 examples [00:00, 43863.37 examples/s]Converting format of dataset (num_proc=16): 65774 examples [00:00, 48536.75 examples/s]Converting format of dataset (num_proc=16): 69672 examples [00:01, 31302.08 examples/s]
[INFO|2025-12-08 12:41:06] llamafactory.data.loader:143 >> Loading dataset ./LVVis/train/qwen3/multi_image_single_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 8709/8709 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 8883 examples [00:00, 446.41 examples/s]          Converting format of dataset (num_proc=16): 14384 examples [00:00, 15077.96 examples/s]Converting format of dataset (num_proc=16): 17418 examples [00:00, 12817.63 examples/s]
[INFO|2025-12-08 12:41:07] llamafactory.data.loader:143 >> Loading dataset ./LVVis/train/qwen3/multi_image_multi_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 7082/7082 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 7216 examples [00:00, 353.05 examples/s]          Converting format of dataset (num_proc=16): 11770 examples [00:00, 12603.95 examples/s]Converting format of dataset (num_proc=16): 14164 examples [00:00, 10489.27 examples/s]
[INFO|2025-12-08 12:41:07] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2021/train/qwen3/single_image/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 10604/10604 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 10728 examples [00:00, 304.21 examples/s]           Converting format of dataset (num_proc=16): 14599 examples [00:00, 10263.35 examples/s]Converting format of dataset (num_proc=16): 18391 examples [00:00, 17729.57 examples/s]Converting format of dataset (num_proc=16): 21208 examples [00:00, 12866.69 examples/s]
[INFO|2025-12-08 12:41:08] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2021/train/qwen3/multi_image_single_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 2651/2651 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 2734 examples [00:00, 208.89 examples/s]          Converting format of dataset (num_proc=16): 5302 examples [00:00, 4224.12 examples/s]
[INFO|2025-12-08 12:41:09] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2021/train/qwen3/multi_image_multi_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 3568/3568 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 3612 examples [00:00, 112.68 examples/s]          Converting format of dataset (num_proc=16): 5168 examples [00:00, 4246.01 examples/s]Converting format of dataset (num_proc=16): 6830 examples [00:00, 7638.94 examples/s]Converting format of dataset (num_proc=16): 7136 examples [00:00, 4706.60 examples/s]
[INFO|2025-12-08 12:41:10] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2022/train/qwen3/single_image/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 10604/10604 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 10721 examples [00:00, 308.93 examples/s]           Converting format of dataset (num_proc=16): 14837 examples [00:00, 11369.11 examples/s]Converting format of dataset (num_proc=16): 18903 examples [00:00, 19511.48 examples/s]Converting format of dataset (num_proc=16): 21208 examples [00:00, 13620.01 examples/s]
[INFO|2025-12-08 12:41:11] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2022/train/qwen3/multi_image_single_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 2651/2651 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 2760 examples [00:00, 278.22 examples/s]          Converting format of dataset (num_proc=16): 5302 examples [00:00, 3763.30 examples/s]
[INFO|2025-12-08 12:41:12] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2022/train/qwen3/multi_image_multi_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 3568/3568 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 3614 examples [00:00, 121.22 examples/s]          Converting format of dataset (num_proc=16): 5180 examples [00:00, 4354.64 examples/s]Converting format of dataset (num_proc=16): 7089 examples [00:00, 8460.20 examples/s]Converting format of dataset (num_proc=16): 7136 examples [00:01, 3169.89 examples/s]
[INFO|2025-12-08 12:41:13] llamafactory.data.loader:143 >> Loading dataset ./OVIS/val/qwen3/single_image/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 80/80 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 85 examples [00:00, 14.21 examples/s]         Converting format of dataset (num_proc=16): 160 examples [00:00, 147.81 examples/s]
[INFO|2025-12-08 12:41:14] llamafactory.data.loader:143 >> Loading dataset ./OVIS/val/qwen3/multi_image_single_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 20/20 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 22 examples [00:00,  5.42 examples/s]         Converting format of dataset (num_proc=16): 40 examples [00:00, 36.14 examples/s]
[INFO|2025-12-08 12:41:15] llamafactory.data.loader:143 >> Loading dataset ./OVIS/val/qwen3/multi_image_multi_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 626/626 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 666 examples [00:00, 102.86 examples/s]         Converting format of dataset (num_proc=16): 1252 examples [00:00, 1092.81 examples/s]
[INFO|2025-12-08 12:41:16] llamafactory.data.loader:143 >> Loading dataset ./LVVis/val/qwen3/single_image/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 8356/8356 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 8538 examples [00:00, 461.90 examples/s]          Converting format of dataset (num_proc=16): 13766 examples [00:00, 14234.65 examples/s]Converting format of dataset (num_proc=16): 16712 examples [00:00, 9409.94 examples/s] 
[INFO|2025-12-08 12:41:17] llamafactory.data.loader:143 >> Loading dataset ./LVVis/val/qwen3/multi_image_single_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 2089/2089 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 2220 examples [00:00, 343.88 examples/s]          Converting format of dataset (num_proc=16): 4178 examples [00:00, 3725.44 examples/s]
[INFO|2025-12-08 12:41:17] llamafactory.data.loader:143 >> Loading dataset ./LVVis/val/qwen3/multi_image_multi_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 1578/1578 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 1677 examples [00:00, 251.21 examples/s]          Converting format of dataset (num_proc=16): 3156 examples [00:00, 2660.40 examples/s]
[INFO|2025-12-08 12:41:18] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2021/val/qwen3/single_image/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 1476/1476 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 1569 examples [00:00, 243.77 examples/s]          Converting format of dataset (num_proc=16): 2952 examples [00:00, 2642.25 examples/s]
[INFO|2025-12-08 12:41:19] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2021/val/qwen3/multi_image_single_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 369/369 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 393 examples [00:00, 62.84 examples/s]          Converting format of dataset (num_proc=16): 738 examples [00:00, 641.43 examples/s]
[INFO|2025-12-08 12:41:20] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2021/val/qwen3/multi_image_multi_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 519/519 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 552 examples [00:00, 85.71 examples/s]          Converting format of dataset (num_proc=16): 1038 examples [00:00, 905.15 examples/s]
[INFO|2025-12-08 12:41:20] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2022/val/qwen3/single_image/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 1676/1676 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 1781 examples [00:00, 253.13 examples/s]          Converting format of dataset (num_proc=16): 3352 examples [00:00, 2793.66 examples/s]
[INFO|2025-12-08 12:41:21] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2022/val/qwen3/multi_image_single_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 419/419 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 446 examples [00:00, 70.57 examples/s]          Converting format of dataset (num_proc=16): 838 examples [00:00, 754.09 examples/s]
[INFO|2025-12-08 12:41:22] llamafactory.data.loader:143 >> Loading dataset ./Youtube-VIS-2022/val/qwen3/multi_image_multi_turn/samples.jsonl...
Converting format of dataset (num_proc=16): 100%|██████████| 715/715 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 760 examples [00:00, 114.41 examples/s]         Converting format of dataset (num_proc=16): 1430 examples [00:00, 1244.12 examples/s]
/home/stud/falu/code/LLaMA-Factory/.venv/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4631: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
[rank0]:[W1208 12:41:23.391907561 ProcessGroupNCCL.cpp:4718] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/87535 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   1%|          | 1000/87535 [00:55<1:20:25, 17.93 examples/s]Running tokenizer on dataset (num_proc=16):   3%|▎         | 3000/87535 [00:56<20:37, 68.32 examples/s]  Running tokenizer on dataset (num_proc=16):   5%|▍         | 4000/87535 [00:56<13:32, 102.88 examples/s]Running tokenizer on dataset (num_proc=16):   6%|▌         | 5000/87535 [00:57<09:07, 150.85 examples/s]Running tokenizer on dataset (num_proc=16):   7%|▋         | 6000/87535 [00:58<06:40, 203.36 examples/s]Running tokenizer on dataset (num_proc=16):   8%|▊         | 7000/87535 [01:06<08:01, 167.19 examples/s]Running tokenizer on dataset (num_proc=16):   9%|▉         | 8000/87535 [01:08<06:22, 208.00 examples/s]Running tokenizer on dataset (num_proc=16):  10%|█         | 9000/87535 [01:09<04:44, 275.81 examples/s]Running tokenizer on dataset (num_proc=16):  11%|█▏        | 10000/87535 [01:10<03:33, 362.39 examples/s]Running tokenizer on dataset (num_proc=16):  11%|█▏        | 10000/87535 [01:28<03:33, 362.39 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 11000/87535 [01:54<19:17, 66.13 examples/s] Running tokenizer on dataset (num_proc=16):  15%|█▍        | 13000/87535 [01:55<10:35, 117.32 examples/s]Running tokenizer on dataset (num_proc=16):  16%|█▌        | 14000/87535 [01:56<07:56, 154.25 examples/s]Running tokenizer on dataset (num_proc=16):  17%|█▋        | 15000/87535 [01:57<06:18, 191.78 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 16000/87535 [01:58<04:37, 257.51 examples/s]Running tokenizer on dataset (num_proc=16):  18%|█▊        | 16000/87535 [02:08<04:37, 257.51 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 17000/87535 [02:11<07:49, 150.38 examples/s]Running tokenizer on dataset (num_proc=16):  21%|██        | 18000/87535 [02:18<07:40, 151.14 examples/s]Running tokenizer on dataset (num_proc=16):  22%|██▏       | 19000/87535 [02:18<05:28, 208.33 examples/s]Running tokenizer on dataset (num_proc=16):  23%|██▎       | 20000/87535 [02:20<04:17, 262.08 examples/s]Running tokenizer on dataset (num_proc=16):  23%|██▎       | 20000/87535 [02:38<04:17, 262.08 examples/s][2025-12-08T12:44:04.417] error: *** JOB 1469427 ON node17 CANCELLED AT 2025-12-08T12:44:04 DUE to SIGNAL Terminated ***
